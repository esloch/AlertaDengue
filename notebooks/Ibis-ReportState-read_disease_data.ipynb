{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import ibis\n",
    "from ibis import config as cf\n",
    "from ibis.sql.postgres import existing_udf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ad_main import settings\n",
    "\n",
    "with cf.config_prefix('sql'):\n",
    "    k = 'default_limit'\n",
    "    cf.set_option(k, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['auth_group',\n",
       " 'auth_group_permissions',\n",
       " 'auth_permission',\n",
       " 'auth_user',\n",
       " 'auth_user_groups',\n",
       " 'auth_user_user_permissions',\n",
       " 'chunked_upload_chunkedupload',\n",
       " 'dbf_dbf',\n",
       " 'dbf_dbfchunkedupload',\n",
       " 'django_admin_log',\n",
       " 'django_content_type',\n",
       " 'django_migrations',\n",
       " 'django_plotly_dash_dashapp',\n",
       " 'django_plotly_dash_statelessapp',\n",
       " 'django_session',\n",
       " 'geography_columns',\n",
       " 'geometry_columns',\n",
       " 'raster_columns',\n",
       " 'raster_overviews',\n",
       " 'spatial_ref_sys',\n",
       " 'teste',\n",
       " 'uf_total_chik_view',\n",
       " 'uf_total_view',\n",
       " 'uf_total_zika_view']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = ibis.postgres.connect(\n",
    "    host=settings.PSQL_HOST,\n",
    "    port=settings.PSQL_PORT,\n",
    "    user=settings.PSQL_USER,\n",
    "    password=settings.PSQL_PASSWORD,\n",
    "    database=settings.PSQL_DB\n",
    ")\n",
    "con.list_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISEASES_SHORT = ['dengue', 'chik', 'zika']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CID10', 'Municipio', 'estado', 'parameters', 'regional_saude']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_dengue_global = con.schema('Dengue_global')\n",
    "schema_dengue_global.list_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bairro',\n",
       " 'Clima_Satelite',\n",
       " 'Clima_cemaden',\n",
       " 'Clima_wu',\n",
       " 'Estacao_cemaden',\n",
       " 'Estacao_wu',\n",
       " 'Historico_alerta',\n",
       " 'Historico_alerta_chik',\n",
       " 'Historico_alerta_zika',\n",
       " 'Localidade',\n",
       " 'Notificacao',\n",
       " 'Ovitrampa',\n",
       " 'Tweet',\n",
       " 'alerta_mrj',\n",
       " 'alerta_mrj_chik',\n",
       " 'alerta_mrj_zika',\n",
       " 'historico_casos']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_city = con.schema('Municipio')\n",
    "schema_city.list_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nome</th>\n",
       "      <th>uf</th>\n",
       "      <th>geocodigo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MINAS GERAIS</td>\n",
       "      <td>mg</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PERNAMBUCO</td>\n",
       "      <td>pe</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALAGOAS</td>\n",
       "      <td>al</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MARANHÃO</td>\n",
       "      <td>ma</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RONDÔNIA</td>\n",
       "      <td>ro</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           nome  uf  geocodigo\n",
       "0  MINAS GERAIS  mg         31\n",
       "1    PERNAMBUCO  pe         26\n",
       "2       ALAGOAS  al         27\n",
       "3      MARANHÃO  ma         21\n",
       "4      RONDÔNIA  ro         11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_estado = schema_dengue_global.table('estado')\n",
    "t_estado[t_estado.nome, t_estado.uf, t_estado.geocodigo].head().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ref_0\n",
       "PostgreSQLTable[table]\n",
       "  name: Municipio\n",
       "  schema:\n",
       "    geocodigo : int32\n",
       "    nome : string\n",
       "    geojson : string\n",
       "    populacao : int64\n",
       "    uf : string\n",
       "\n",
       "Selection[table]\n",
       "  table:\n",
       "    Table: ref_0\n",
       "  selections:\n",
       "    geocodigo = Column[int32*] 'geocodigo' from table\n",
       "      ref_0\n",
       "    nome = Column[string*] 'nome' from table\n",
       "      ref_0\n",
       "  predicates:\n",
       "    Equals[boolean*]\n",
       "      left:\n",
       "        uf = Column[string*] 'uf' from table\n",
       "          ref_0\n",
       "      right:\n",
       "        Literal[string]\n",
       "          Ceará"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preare cities dictionary\n",
    "t_cities = schema_dengue_global.table('Municipio')\n",
    "\n",
    "uf_filter_ceara = t_cities.uf == 'Ceará'\n",
    "keys = [t_cities.geocodigo, t_cities.nome]\n",
    "\n",
    "expr_cities = t_cities[uf_filter_ceara][keys]\n",
    "\n",
    "df_cities = expr_cities.execute().set_index('geocodigo')\n",
    "cities = df_cities.to_dict()['nome']\n",
    "\n",
    "expr_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{2305233: 'Horizonte', 2305266: 'Ibaretama', 23053...\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(cities)[:50] + '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PostgreSQLTable[table]\n",
       "  name: regional_saude\n",
       "  schema:\n",
       "    id : int32\n",
       "    municipio_geocodigo : int32\n",
       "    id_regional : int32\n",
       "    codigo_estacao_wu : string\n",
       "    nome_regional : string\n",
       "    limiar_preseason : float32\n",
       "    limiar_posseason : float32\n",
       "    limiar_epidemico : float32\n",
       "    estacao_wu_sec : string\n",
       "    varcli : string\n",
       "    tcrit : float64\n",
       "    ucrit : float64\n",
       "    nome_macroreg : string"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare station_id\n",
    "t_rsaude = schema_dengue_global.table('regional_saude')\n",
    "t_rsaude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   municipio_geocodigo codigo_estacao_wu    varcli     uf         nome\n",
      "0              2305233              SBFZ  umid_max  Ceará    Horizonte\n",
      "1              2305266              SBTE  umid_max  Ceará    Ibaretama\n",
      "2              2305308              SBFZ  umid_max  Ceará     Ibiapina\n",
      "3              2305332              SBTE  umid_max  Ceará  Ibicuitinga\n",
      "4              2305357              SBFZ  umid_max  Ceará       Icapuí\n"
     ]
    }
   ],
   "source": [
    "rsaude_keys = [\n",
    "    t_rsaude.municipio_geocodigo,\n",
    "    t_rsaude.codigo_estacao_wu,\n",
    "    t_rsaude.varcli,\n",
    "    t_cities.uf,\n",
    "    t_cities.nome\n",
    "]\n",
    "\n",
    "expr_rsaude = t_rsaude.join(\n",
    "    t_cities, \n",
    "    (t_rsaude.municipio_geocodigo == t_cities.geocodigo) \n",
    "    & (t_cities.uf == 'Ceará')\n",
    ")\n",
    "print(expr_rsaude[rsaude_keys].head().execute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_climate = 'umid_max'\n",
    "year_week = 202002\n",
    "station_id = 'SBFZ'\n",
    "\n",
    "general_param = {\n",
    "    'year_week_start': year_week - 200,\n",
    "    'year_week_end': year_week,\n",
    "    'geocodes': ','.join(map(lambda v: repr(str(v)), cities)),\n",
    "    'geocodes_list': [v for v in cities],\n",
    "    'var_climate': var_climate,\n",
    "    'station_id': station_id,\n",
    "}\n",
    "\n",
    "disease = 'dengue'\n",
    "\n",
    "_param = dict(general_param)\n",
    "_param['disease'] = disease\n",
    "\n",
    "table_suffix = ''\n",
    "if disease != 'dengue':\n",
    "    table_suffix = '_{}'.format(disease)\n",
    "\n",
    "_param['table_suffix'] = table_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ref_0\n",
       "PostgreSQLTable[table]\n",
       "  name: Tweet\n",
       "  schema:\n",
       "    id : int64\n",
       "    Municipio_geocodigo : int32\n",
       "    data_dia : date\n",
       "    numero : int32\n",
       "    CID10_codigo : string\n",
       "\n",
       "ref_1\n",
       "Selection[table]\n",
       "  table:\n",
       "    Table: ref_0\n",
       "  predicates:\n",
       "    Contains[boolean*]\n",
       "      value:\n",
       "        Municipio_geocodigo = Column[int32*] 'Municipio_geocodigo' from table\n",
       "          ref_0\n",
       "      options:\n",
       "        Literal[set<int32>]\n",
       "          frozenset({2308609, 2304004, 2308104, 2303501, 2312205, 2307601, 2311702, 2313757, 2303006, 2307106, 2313252, 2302503, 2311207, 2306603, 2310704, 2307635, 2302008, 2306108, 2311231, 2301505, 2307650, 2310209, 2305605, 2301000, 2309706, 2305100, 2313807, 2309201, 2313302, 2300507, 2304608, 2311264, 2308708, 2304103, 2302057, 2312809, 2308203, 2303600, 2312304, 2310258, 2307700, 2305654, 2311801, 2303105, 2307205, 2313351, 2302602, 2311306, 2306702, 2304657, 2310803, 2302107, 2306207, 2301604, 2310308, 2305704, 2303659, 2309805, 2313906, 2309300, 2301109, 2307254, 2305209, 2313401, 2311355, 2300606, 2304707, 2310852, 2300101, 2308807, 2304202, 2312908, 2308302, 2306256, 2305233, 2312403, 2311900, 2303709, 2307809, 2313955, 2303204, 2307304, 2304236, 2302701, 2311405, 2306801, 2305266, 2300150, 2310902, 2304251, 2302206, 2308351, 2306306, 2301703, 2310407, 2305803, 2304269, 2309904, 2314003, 2304277, 2311959, 2301208, 2308377, 2305308, 2304285, 2313500, 2300705, 2309409, 2304806, 2310951, 2300200, 2308906, 2304301, 2313005, 2308401, 2305332, 2312502, 2303808, 2307908, 2303303, 2312007, 2301257, 2307403, 2305357, 2302800, 2311504, 2300754, 2309458, 2306900, 2313559, 2304350, 2302305, 2311009, 2306405, 2301802, 2310506, 2305902, 2310001, 2314102, 2301307, 2305407, 2300804, 2309508, 2304905, 2313609, 2309003, 2304400, 2313104, 2308500, 2300309, 2312601, 2301851, 2303907, 2308005, 2303402, 2312106, 2307502, 2311603, 2304954, 2303931, 2302909, 2307007, 2302404, 2311108, 2306504, 2304459, 2301901, 2310605, 2303956, 2310100, 2306009, 2301406, 2305506, 2300903, 2309607, 2305001, 2313708, 2309102, 2313203, 2300408, 2306553, 2312700, 2304509, 2301950})\n",
       "\n",
       "ref_2\n",
       "Selection[table]\n",
       "  table:\n",
       "    Table: ref_1\n",
       "  selections:\n",
       "    Table: ref_1\n",
       "    SE_twitter = epi_week_0[int32*]\n",
       "      v0:\n",
       "        data_dia = Column[date*] 'data_dia' from table\n",
       "          ref_1\n",
       "\n",
       "Aggregation[table]\n",
       "  table:\n",
       "    Table: ref_2\n",
       "  metrics:\n",
       "    n_tweets = Sum[int64]\n",
       "      numero = Column[int32*] 'numero' from table\n",
       "        ref_2\n",
       "      where:\n",
       "        None\n",
       "  by:\n",
       "    SE_twitter = Column[int32*] 'SE_twitter' from table\n",
       "      ref_2\n",
       "    Municipio_geocodigo = Column[int32*] 'Municipio_geocodigo' from table\n",
       "      ref_2\n",
       "  predicates:\n",
       "    Between[boolean*]\n",
       "      SE_twitter = Column[int32*] 'SE_twitter' from table\n",
       "        ref_2\n",
       "      lower_bound:\n",
       "        Literal[int32]\n",
       "          201802\n",
       "      upper_bound:\n",
       "        Literal[int32]\n",
       "          202002\n",
       "  sort_keys:\n",
       "    SortKey[array-sort]\n",
       "      expr:\n",
       "        SE_twitter = Column[int32*] 'SE_twitter' from table\n",
       "          ref_2\n",
       "      ascending:\n",
       "        False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epi_week_fn = existing_udf(\n",
    "    'epi_week',\n",
    "    input_types=['date'],\n",
    "    output_type='int32'\n",
    ")\n",
    "\n",
    "t_tweet = schema_city.table('Tweet')\n",
    "filter_tweet_cities = t_tweet.Municipio_geocodigo.isin(\n",
    "    general_param['geocodes_list']\n",
    ")\n",
    "t_tweet = t_tweet[filter_tweet_cities]\n",
    "t_tweet = t_tweet.mutate(SE_twitter=epi_week_fn(t_tweet.data_dia))\n",
    "\n",
    "expr_tweet = t_tweet.groupby([\n",
    "    t_tweet.SE_twitter,\n",
    "    t_tweet.Municipio_geocodigo,\n",
    "    \n",
    "]).aggregate(\n",
    "    n_tweets=t_tweet.numero.sum()\n",
    ")\n",
    "\n",
    "filter_tweet = (\n",
    "    t_tweet.SE_twitter.between(\n",
    "        general_param['year_week_start'], \n",
    "        general_param['year_week_end']\n",
    "    )\n",
    ")\n",
    "\n",
    "# ORDER BY \"SE_twitter\" DESC\n",
    "expr_tweet = expr_tweet[filter_tweet].sort_by(('SE_twitter', False))\n",
    "expr_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 344 ms, sys: 15.6 ms, total: 359 ms\n",
      "Wall time: 5.23 s\n"
     ]
    }
   ],
   "source": [
    "def get_epi_week_expr():\n",
    "    return existing_udf(\n",
    "        'epi_week',\n",
    "        input_types=['date'],\n",
    "        output_type='int64'\n",
    "    )\n",
    "\n",
    "def get_epiweek2date_expr():\n",
    "    return existing_udf(\n",
    "        'epiweek2date',\n",
    "        input_types=['int64'],\n",
    "        output_type='date'\n",
    "    )\n",
    "\n",
    "\n",
    "def get_hist_disease_expr(\n",
    "    disease: str,\n",
    "    geocodes: List[int],\n",
    "    year_week_start: int,\n",
    "    year_week_end: int,\n",
    "):\n",
    "    table_suffix = ''\n",
    "    if disease != 'dengue':\n",
    "        table_suffix = '_{}'.format(disease)\n",
    "    \n",
    "    schema_city = con.schema('Municipio')\n",
    "    t_hist = schema_city.table('Historico_alerta{}'.format(table_suffix))\n",
    "\n",
    "    case_level = (\n",
    "        ibis.case()\n",
    "        .when((t_hist.nivel.cast('string') == '1'), 'verde')\n",
    "        .when((t_hist.nivel.cast('string') == '2'), 'amarelo')\n",
    "        .when((t_hist.nivel.cast('string') == '3'), 'laranja')\n",
    "        .when((t_hist.nivel.cast('string') == '4'), 'vermelho')\n",
    "        .else_('-')\n",
    "        .end()\n",
    "    ).name(f'nivel_{disease}')\n",
    "\n",
    "    hist_keys = [\n",
    "        t_hist.SE.name(f'SE_{disease}'),\n",
    "        t_hist.casos.name(f'casos_{disease}'),\n",
    "        t_hist.p_rt1.name(f'p_rt1_{disease}'),\n",
    "        t_hist.casos_est.name(f'casos_est_{disease}'),\n",
    "        t_hist.p_inc100k.name(f'p_inc100k_{disease}'),\n",
    "        t_hist.nivel.name(f'level_code_{disease}'),\n",
    "        case_level,\n",
    "        t_hist.municipio_geocodigo.name(f'geocode_{disease}')\n",
    "    ]\n",
    "\n",
    "    hist_filter = (\n",
    "        (t_hist['SE'].between(year_week_start, year_week_end))\n",
    "        & (t_hist['municipio_geocodigo'].isin(geocodes))\n",
    "    )\n",
    "\n",
    "    return t_hist[hist_filter][hist_keys].sort_by(f'SE_{disease}')\n",
    "\n",
    "\n",
    "def get_twitter_expr(\n",
    "    geocodes_list: List[int], \n",
    "    year_week_start: int,\n",
    "    year_week_end: int\n",
    "):\n",
    "    epi_week_fn = get_epi_week_expr()\n",
    "    \n",
    "    t_tweet = schema_city.table('Tweet')\n",
    "    filter_tweet_cities = t_tweet.Municipio_geocodigo.isin(\n",
    "        geocodes_list\n",
    "    )\n",
    "    t_tweet = t_tweet[filter_tweet_cities]\n",
    "    t_tweet = t_tweet.mutate(SE_twitter=epi_week_fn(t_tweet.data_dia))\n",
    "\n",
    "    expr_tweet = t_tweet.groupby([\n",
    "        t_tweet.SE_twitter,\n",
    "        t_tweet.Municipio_geocodigo,\n",
    "\n",
    "    ]).aggregate(\n",
    "        n_tweets=t_tweet.numero.sum()\n",
    "    )\n",
    "\n",
    "    filter_tweet = (\n",
    "        t_tweet.SE_twitter.between(\n",
    "            general_param['year_week_start'], \n",
    "            general_param['year_week_end']\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return expr_tweet[filter_tweet].sort_by(('SE_twitter', False))\n",
    "\n",
    "\n",
    "def get_climate_wu_expr(var_climate: str, station_id: str):\n",
    "    epi_week_fn = get_epi_week_expr()\n",
    "    \n",
    "    schema_city = con.schema('Municipio')\n",
    "    \n",
    "    t_climate_wu = schema_city.table('Clima_wu')\n",
    "    \n",
    "    expr_filter = t_climate_wu.Estacao_wu_estacao_id == station_id\n",
    "    t_climate_wu = t_climate_wu[expr_filter]\n",
    "    t_climate_wu = t_climate_wu.mutate(\n",
    "        epiweek_climate=epi_week_fn(t_climate_wu.data_dia)\n",
    "    )\n",
    "    \n",
    "    expr_climate_wu = t_climate_wu.groupby([\n",
    "        t_climate_wu.epiweek_climate\n",
    "    ]).aggregate(\n",
    "        **{var_climate: t_climate_wu[var_climate].mean().name(var_climate)}\n",
    "    )\n",
    "    \n",
    "    return expr_climate_wu.sort_by(t_climate_wu.epiweek_climate)\n",
    "    \n",
    "    \n",
    "hist_disease_expr = {}\n",
    "hist_prev = None\n",
    "joined_expr = None\n",
    "previous_disease = None\n",
    "\n",
    "for disease in DISEASES_SHORT:\n",
    "    hist_expr = get_hist_disease_expr(\n",
    "        disease, \n",
    "        general_param['geocodes_list'],\n",
    "        general_param['year_week_start'],\n",
    "        general_param['year_week_end'],\n",
    "    )\n",
    "    hist_disease_expr[disease] = hist_expr\n",
    "    \n",
    "    if joined_expr is None:\n",
    "        joined_expr = hist_expr\n",
    "    else:\n",
    "        # todo: review join approach\n",
    "        hist_prev = hist_disease_expr[previous_disease]\n",
    "        joined_cond = (\n",
    "            (hist_prev[f'SE_{previous_disease}'] == hist_expr[f'SE_{disease}'])\n",
    "            & (hist_prev[f'geocode_{previous_disease}'] == hist_expr[f'geocode_{disease}'])\n",
    "        )\n",
    "        joined_expr = joined_expr.left_join(\n",
    "            hist_expr,\n",
    "            joined_cond\n",
    "        )\n",
    "    previous_disease = disease\n",
    "\n",
    "# twitter data\n",
    "expr_tweet = get_twitter_expr(\n",
    "    general_param['geocodes_list'],\n",
    "    general_param['year_week_start'],\n",
    "    general_param['year_week_end'],\n",
    ")\n",
    "expr_dengue = hist_disease_expr['dengue']\n",
    "\n",
    "joined_expr = joined_expr.left_join(\n",
    "    expr_tweet, \n",
    "    (\n",
    "        (\n",
    "            expr_tweet.Municipio_geocodigo == expr_dengue.geocode_dengue\n",
    "        ) & (\n",
    "            expr_tweet.SE_twitter == expr_dengue.SE_dengue\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "climate_wu_expr = get_climate_wu_expr(\n",
    "    general_param['var_climate'], \n",
    "    general_param['station_id']\n",
    ")\n",
    "\n",
    "joined_expr = joined_expr.left_join(\n",
    "    climate_wu_expr, \n",
    "    hist_disease_expr['dengue'].SE_dengue == climate_wu_expr.epiweek_climate\n",
    ").materialize()\n",
    "\n",
    "epiweek2date_fn = get_epiweek2date_expr()\n",
    "\n",
    "k = ['casos', 'p_inc100k', 'casos_est', 'p_rt1', 'nivel', 'level_code']\n",
    "proj = [joined_expr['{}_{}'.format(v, d)] for v in k for d in DISEASES_SHORT]\n",
    "proj.extend([\n",
    "    joined_expr[var_climate],\n",
    "    joined_expr['n_tweets'],\n",
    "    joined_expr['geocode_dengue'].name('geocode'),\n",
    "    joined_expr['SE_dengue'].name('SE'),\n",
    "    epiweek2date_fn(joined_expr.SE_dengue).name('init_date_week'),\n",
    "])\n",
    "\n",
    "%time df_ibis = joined_expr[proj].execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_read_disease_data(\n",
    "    cities: dict, station_id: str, year_week: int, var_climate: str\n",
    ") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        :param cities:\n",
    "        :param station_id:\n",
    "        :param year_week:\n",
    "        :param var_climate:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        DISEASES = ['dengue', 'chik', 'zika']\n",
    "\n",
    "        k = ['casos', 'p_inc100k', 'casos_est', 'p_rt1', 'nivel', 'level_code']\n",
    "\n",
    "        k = ['{}_{}'.format(v, d) for v in k for d in DISEASES]\n",
    "\n",
    "        k.append(var_climate)\n",
    "        k.append('n_tweets')\n",
    "        k.append('geocode_dengue AS geocode')\n",
    "        k.append('\"SE_dengue\" AS \"SE\"')\n",
    "        k.append('epiweek2date(\"SE_dengue\") AS init_date_week')\n",
    "\n",
    "        general_param = {\n",
    "            'year_week_start': year_week - 200,\n",
    "            'year_week_end': year_week,\n",
    "            'geocodes': ','.join(map(lambda v: \"'{}'\".format(v), cities)),\n",
    "            'var_climate': var_climate,\n",
    "            'station_id': station_id,\n",
    "        }\n",
    "\n",
    "        sql = ''\n",
    "        previous_disease = ''\n",
    "        for disease in DISEASES:\n",
    "            _param = dict(general_param)\n",
    "            _param['disease'] = disease\n",
    "\n",
    "            table_suffix = ''\n",
    "            if disease != 'dengue':\n",
    "                table_suffix = '_{}'.format(disease)\n",
    "\n",
    "            _param['table_suffix'] = table_suffix\n",
    "\n",
    "            sql_ = (\n",
    "                '''\n",
    "            (SELECT\n",
    "               hist.\"SE\" AS \"SE_%(disease)s\",\n",
    "               hist.casos AS casos_%(disease)s,\n",
    "               hist.p_rt1 AS p_rt1_%(disease)s,\n",
    "               hist.casos_est AS casos_est_%(disease)s,\n",
    "               hist.p_inc100k AS p_inc100k_%(disease)s,\n",
    "               hist.nivel AS level_code_%(disease)s,\n",
    "               (CASE\n",
    "                  WHEN hist.nivel=1 THEN 'verde'\n",
    "                  WHEN hist.nivel=2 THEN 'amarelo'\n",
    "                  WHEN hist.nivel=3 THEN 'laranja'\n",
    "                  WHEN hist.nivel=4 THEN 'vermelho'\n",
    "                  ELSE '-'\n",
    "                END) AS nivel_%(disease)s,\n",
    "                hist.municipio_geocodigo AS geocode_%(disease)s\n",
    "            FROM\n",
    "             \"Municipio\".\"Historico_alerta%(table_suffix)s\" AS hist\n",
    "            WHERE\n",
    "             hist.\"SE\" BETWEEN %(year_week_start)s AND %(year_week_end)s\n",
    "             AND hist.municipio_geocodigo IN (%(geocodes)s)\n",
    "            ORDER BY \"SE_%(disease)s\" DESC\n",
    "            ) AS %(disease)s\n",
    "            '''\n",
    "                % _param\n",
    "            )\n",
    "\n",
    "            if not sql:\n",
    "                sql = sql_\n",
    "            else:\n",
    "                sql += '''\n",
    "                    LEFT JOIN {0}\n",
    "                    ON (\n",
    "                      {1}.\"SE_{1}\" = {2}.\"SE_{2}\"\n",
    "                      AND {1}.geocode_{1} = {2}.geocode_{2}\n",
    "                    )\n",
    "                '''.format(\n",
    "                    sql_, previous_disease, disease\n",
    "                )\n",
    "            previous_disease = disease\n",
    "\n",
    "        tweet_join = (\n",
    "            '''\n",
    "        LEFT JOIN (\n",
    "           SELECT\n",
    "             epi_week(data_dia) AS \"SE_twitter\",\n",
    "             SUM(numero) as n_tweets,\n",
    "             \"Municipio_geocodigo\"\n",
    "           FROM \"Municipio\".\"Tweet\"\n",
    "           WHERE\n",
    "             \"Municipio_geocodigo\" IN (%(geocodes)s)\n",
    "             AND epi_week(data_dia)\n",
    "               BETWEEN %(year_week_start)s AND %(year_week_end)s\n",
    "           GROUP BY \"SE_twitter\", \"Municipio_geocodigo\"\n",
    "           ORDER BY \"SE_twitter\" DESC\n",
    "        ) AS tweets\n",
    "           ON (\n",
    "             \"Municipio_geocodigo\"=dengue.\"geocode_dengue\"\n",
    "             AND tweets.\"SE_twitter\"=dengue.\"SE_dengue\"\n",
    "           )\n",
    "        '''\n",
    "            % general_param\n",
    "        )\n",
    "\n",
    "        climate_join = (\n",
    "            '''\n",
    "        LEFT JOIN (\n",
    "          SELECT\n",
    "             epi_week(data_dia) AS epiweek_climate,\n",
    "             AVG(%(var_climate)s) AS %(var_climate)s\n",
    "          FROM \"Municipio\".\"Clima_wu\"\n",
    "          WHERE \"Estacao_wu_estacao_id\" = '%(station_id)s'\n",
    "          GROUP BY epiweek_climate\n",
    "          ORDER BY epiweek_climate\n",
    "        ) AS climate_wu\n",
    "           ON (dengue.\"SE_dengue\"=climate_wu.epiweek_climate)\n",
    "        '''\n",
    "            % general_param\n",
    "        )\n",
    "\n",
    "        sql += climate_join + tweet_join\n",
    "\n",
    "        sql = ' SELECT {} FROM ({}) AS data'.format(','.join(k), sql)\n",
    "        df = pd.read_sql(sql, index_col='SE', con=con.con)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 111 ms, sys: 27.9 ms, total: 138 ms\n",
      "Wall time: 6.2 s\n"
     ]
    }
   ],
   "source": [
    "%time df_sql = sql_read_disease_data(cities, station_id, year_week, var_climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.init_date_week.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df_sql = df_sql.copy()\n",
    "_df_sql = _df_sql.astype({'init_date_week': 'datetime64[ns]'})\n",
    "_df_sql.init_date_week.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "DataFrame.index are different\n\nDataFrame.index values are different (98.95445 %)\n[left]:  Int64Index([202002, 202002, 202002, 202002, 202002, 202002, 202002, 202002,\n            202002, 202002,\n            ...\n            201809, 201832, 201806, 201849, 201926, 201828, 201846, 201837,\n            201902, 201920],\n           dtype='int64', name='SE', length=19320)\n[right]: Int64Index([201802, 201802, 201802, 201802, 201802, 201802, 201802, 201802,\n            201802, 201802,\n            ...\n            202002, 202002, 202002, 202002, 202002, 202002, 202002, 202002,\n            202002, 202002],\n           dtype='int64', name='SE', length=19320)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9415786dab88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0m_df_sql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_date_week\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_df_sql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_date_week\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pydatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_frame_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_df_ibis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_df_sql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/testing.pyx\u001b[0m in \u001b[0;36mpandas._libs.testing.assert_almost_equal\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/testing.pyx\u001b[0m in \u001b[0;36mpandas._libs.testing.assert_almost_equal\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/docker-env/miniconda/envs/alerta-dengue/lib/python3.7/site-packages/pandas/_testing.py\u001b[0m in \u001b[0;36mraise_assert_detail\u001b[0;34m(obj, message, left, right, diff, index_values)\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"\\n[diff]: {diff}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: DataFrame.index are different\n\nDataFrame.index values are different (98.95445 %)\n[left]:  Int64Index([202002, 202002, 202002, 202002, 202002, 202002, 202002, 202002,\n            202002, 202002,\n            ...\n            201809, 201832, 201806, 201849, 201926, 201828, 201846, 201837,\n            201902, 201920],\n           dtype='int64', name='SE', length=19320)\n[right]: Int64Index([201802, 201802, 201802, 201802, 201802, 201802, 201802, 201802,\n            201802, 201802,\n            ...\n            202002, 202002, 202002, 202002, 202002, 202002, 202002, 202002,\n            202002, 202002],\n           dtype='int64', name='SE', length=19320)"
     ]
    }
   ],
   "source": [
    "# set index\n",
    "_df_ibis = df_ibis.set_index('SE', drop=True)\n",
    "# use the same data type for both dataframes\n",
    "_df_ibis = _df_ibis.astype({\n",
    "    col_name: col_type for col_name, col_type in df_sql.dtypes.items()\n",
    "})\n",
    "# reformat datatype\n",
    "_df_ibis.init_date_week = _df_ibis.init_date_week.apply(lambda v: v.to_pydatetime())\n",
    "_df_sql.init_date_week = _df_sql.init_date_week.apply(lambda v: v.to_pydatetime())\n",
    "# test\n",
    "pd.testing.assert_frame_equal(_df_ibis, _df_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "def episem(x, sep='W', out='YW'):\n",
    "\n",
    "    \"\"\"\n",
    "    Return Brazilian corresponding epidemiological week from x.\n",
    "\n",
    "    :param x: Input date. Can be a string in the format %Y-%m-%d or\n",
    "      datetime.datetime\n",
    "    :param sep: Year and week separator.\n",
    "    :param out: Output format. 'YW' returns sep.join(epiyear,epiweek).\n",
    "     'Y' returns epiyear only. 'W' returns epiweek only.\n",
    "    :return: str\n",
    "    \"\"\"\n",
    "\n",
    "    def out_format(year, week, out, sep='W'):\n",
    "        if out == 'YW':\n",
    "            return '%s%s%02d' % (year, sep, week)\n",
    "        if out == 'Y':\n",
    "            return '%s' % (year)\n",
    "        if out == 'W':\n",
    "            return '%02d' % week\n",
    "\n",
    "    if type(x) != datetime.datetime:\n",
    "        if str(x) == '' or x is None or (type(x) != str and np.isnan(x)):\n",
    "            return None\n",
    "        x = datetime.datetime.strptime(x, '%Y-%m-%d')\n",
    "\n",
    "    epiyear = x.year\n",
    "    epiend = lastepiday(epiyear)\n",
    "\n",
    "    if x > epiend:\n",
    "        epiyear += 1\n",
    "        return out_format(epiyear, 1, out, sep)\n",
    "\n",
    "    epistart = firstepiday(epiyear)\n",
    "\n",
    "    # If current date is before its year first epiweek,\n",
    "    # then our base year is the previous one\n",
    "    if x < epistart:\n",
    "        epiyear -= 1\n",
    "        epistart = firstepiday(epiyear)\n",
    "\n",
    "    epiweek = int(((x - epistart) / 7).days) + 1\n",
    "\n",
    "    return out_format(epiyear, epiweek, out, sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_original(df):\n",
    "    if not df.empty:\n",
    "        dfs = []\n",
    "\n",
    "        # merge with a range date dataframe to keep empty week on the data\n",
    "        ts_date = pd.date_range(\n",
    "            df['init_date_week'].min(),\n",
    "            df['init_date_week'].max(),\n",
    "            freq='7D',\n",
    "        )\n",
    "        df_date = pd.DataFrame({'init_date_week': ts_date})\n",
    "\n",
    "        for geocode in df.geocode.unique():\n",
    "            df_ = df[df.geocode == geocode].sort_values('init_date_week')\n",
    "\n",
    "            df_date_ = df_date.set_index(\n",
    "                df_.init_date_week.map(\n",
    "                    lambda x: int(episem(str(x)[:10], sep=''))\n",
    "                ),\n",
    "                drop=True,\n",
    "            )\n",
    "\n",
    "            df_.index.name = None\n",
    "            df_date_.index.name = None\n",
    "\n",
    "            df_['init_date_week'] = pd.to_datetime(\n",
    "                df_['init_date_week'], errors='coerce'\n",
    "            )\n",
    "\n",
    "            dfs.append(\n",
    "                pd.merge(\n",
    "                    df_,\n",
    "                    df_date_,\n",
    "                    how='outer',\n",
    "                    on='init_date_week',\n",
    "                    left_index=True,\n",
    "                    right_index=True,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        df = pd.concat(dfs)\n",
    "\n",
    "    df.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "    for d in DISEASES_SHORT:\n",
    "        k = 'p_rt1_{}'.format(d)\n",
    "        df[k] = (df[k] * 100).fillna(0)\n",
    "        k = 'casos_est_{}'.format(d)\n",
    "        df[k] = df[k].fillna(0).round(0)\n",
    "        k = 'p_inc100k_{}'.format(d)\n",
    "        df[k] = df[k].fillna(0).round(0)\n",
    "\n",
    "        df.rename(\n",
    "            columns={\n",
    "                'p_inc100k_{}'.format(d): 'incidência {}'.format(d),\n",
    "                'casos_{}'.format(d): 'casos notif. {}'.format(d),\n",
    "                'casos_est_{}'.format(d): 'casos est. {}'.format(d),\n",
    "                'p_rt1_{}'.format(d): 'pr(incid. subir) {}'.format(d),\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "    df.n_tweets = df.n_tweets.fillna(0).round(0)\n",
    "\n",
    "    return df.rename(\n",
    "        columns={\n",
    "            'umid_max': 'umid.max',\n",
    "            'temp_min': 'temp.min',\n",
    "            'n_tweets': 'tweets',\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df_new(df):\n",
    "    if not df.empty:\n",
    "        dfs = []\n",
    "\n",
    "        # merge with a range date dataframe to keep empty week on the data\n",
    "        ts_date = pd.date_range(\n",
    "            df['init_date_week'].min(),\n",
    "            df['init_date_week'].max(),\n",
    "            freq='7D',\n",
    "        )\n",
    "        df_date = pd.DataFrame({'init_date_week': ts_date})\n",
    "\n",
    "        for geocode in df.geocode.unique():\n",
    "            df_ = df[df.geocode == geocode].sort_values('init_date_week')\n",
    "\n",
    "            df_date_ = df_date.set_index(\n",
    "                df_.init_date_week.map(\n",
    "                    lambda x: int(episem(str(x)[:10], sep=''))\n",
    "                ),\n",
    "                drop=True,\n",
    "            )\n",
    "\n",
    "            df_.index.name = None\n",
    "            df_date_.index.name = None\n",
    "\n",
    "            df_['init_date_week'] = pd.to_datetime(\n",
    "                df_['init_date_week'], errors='coerce'\n",
    "            )\n",
    "\n",
    "            dfs.append(\n",
    "                pd.merge(\n",
    "                    df_,\n",
    "                    df_date_,\n",
    "                    how='outer',\n",
    "                    on='init_date_week',\n",
    "                    left_index=True,\n",
    "                    right_index=True,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        df = pd.concat(dfs)\n",
    "\n",
    "    df.sort_index(ascending=True, inplace=True)\n",
    "\n",
    "    for d in DISEASES_SHORT:\n",
    "        k = 'p_rt1_{}'.format(d)\n",
    "        df[k] = (df[k] * 100).fillna(0)\n",
    "        k = 'casos_est_{}'.format(d)\n",
    "        df[k] = df[k].fillna(0).round(0)\n",
    "        k = 'p_inc100k_{}'.format(d)\n",
    "        df[k] = df[k].fillna(0).round(0)\n",
    "\n",
    "        df.rename(\n",
    "            columns={\n",
    "                'p_inc100k_{}'.format(d): 'incidência {}'.format(d),\n",
    "                'casos_{}'.format(d): 'casos notif. {}'.format(d),\n",
    "                'casos_est_{}'.format(d): 'casos est. {}'.format(d),\n",
    "                'p_rt1_{}'.format(d): 'pr(incid. subir) {}'.format(d),\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "    df.n_tweets = df.n_tweets.fillna(0).round(0)\n",
    "\n",
    "    return df.rename(\n",
    "        columns={\n",
    "            'umid_max': 'umid.max',\n",
    "            'temp_min': 'temp.min',\n",
    "            'n_tweets': 'tweets',\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'p_rt1_dengue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/ibis-nb/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'p_rt1_dengue'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-036255ecb3e2>\u001b[0m in \u001b[0;36mprepare_df_original\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDISEASES_SHORT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'p_rt1_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'casos_est_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ibis-nb/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ibis-nb/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'p_rt1_dengue'"
     ]
    }
   ],
   "source": [
    "%time prepare_df_original(pd.DataFrame())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
